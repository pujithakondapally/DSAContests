{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e81a3e",
   "metadata": {},
   "source": [
    "# Ollama \n",
    "\n",
    "## What is Ollama?\n",
    "Ollama is an **open-source tool** that lets you run **large language models (LLMs)** locally on your computer.  \n",
    "\n",
    "## Why use Ollama?\n",
    "- Instead of relying on cloud APIs (like OpenAI or Anthropic), Ollama brings models **directly to your machine**.  \n",
    "-  More **privacy**  \n",
    "-  Lower **cost**  \n",
    "- Works **offline**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bb1a8",
   "metadata": {},
   "source": [
    "#  Local vs Cloud LLMs\n",
    "\n",
    "| Feature            | Ollama (Local)                           | OpenAI API (Cloud)                 |\n",
    "|--------------------|------------------------------------------|-------------------------------------|\n",
    "| **Deployment**     | Runs fully on your computer              | Runs on OpenAI’s cloud servers      |\n",
    "| **Models**         | Open-source (LLaMA, Mistral, Gemma, etc.)| Proprietary (GPT-4o, GPT-3.5)       |\n",
    "| **Privacy**        | High – data never leaves your machine    | Lower – prompts & data sent to API  |\n",
    "| **Cost**           | Free after download (uses local compute) | Pay-per-token usage                 |\n",
    "| **Performance**    | Depends on your hardware (RAM/VRAM)      | Optimized cloud infrastructure      |\n",
    "| **Offline Support**| Yes                                      | No                                  |\n",
    "| **Ease of Setup**  | Needs installation & model download      | Simple API call (no setup hassle)   |\n",
    "| **Ecosystem**      | Local apps, developer tooling            | Rich API features & integrations    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1006c",
   "metadata": {},
   "source": [
    "Run via Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ed9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def run_client(prompt: str):\n",
    "    response = ollama.generate(model=\"llama3.1:8b\", prompt=prompt)\n",
    "    return response[\"response\"]\n",
    "\n",
    "print(run_client(\"Write a Python function to calculate factorial.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04867b79",
   "metadata": {},
   "source": [
    "Run via REST API (Requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ee6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def run_requests(prompt: str):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": \"llama3.1:8b\", \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "print(run_requests(\"Write a Python function to calculate factorial.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8f07c",
   "metadata": {},
   "source": [
    "Run via LangChain Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcaf34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "def run_langchain(prompt: str):\n",
    "    llm = Ollama(model=\"llama3.1:8b\")\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "print(run_langchain(\"Write a Python function to calculate factorial.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3de54",
   "metadata": {},
   "source": [
    "# Ways to Run Ollama\n",
    "\n",
    "| Method         | Best For                                | Example Use Case                  |\n",
    "|----------------|------------------------------------------|-----------------------------------|\n",
    "| **CLI**        | Quick tests, debugging                   | Run a single prompt in terminal   |\n",
    "| **Python**     | Scripts, notebooks, ML pipelines         | Jupyter demo, automation script   |\n",
    "| **REST API**   | Web apps, microservices, cross-language  | Flask backend, Node.js frontend   |\n",
    "| **LangChain**  | Complex apps, memory, RAG, multi-tools   | Chatbot with docs search          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256241e8",
   "metadata": {},
   "source": [
    "Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22516487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A **linked list** is a linear data structure where each element, called a node, points to the next node in the sequence. This allows for efficient insertion and deletion of elements at any position in the list.\n",
      "\n",
      "Here's a simple representation of a linked list:\n",
      "\n",
      "```\n",
      "  A <-> B <-> C <-> D\n",
      "```\n",
      "\n",
      "In this example:\n",
      "\n",
      "* Each letter (A, B, C, D) represents a **node**, which is an object that contains some data (in this case, a single character).\n",
      "* The arrows (`<->`) represent the **pointers** between nodes. Each node points to the next one in the sequence.\n",
      "\n",
      "Linked lists are useful when:\n",
      "\n",
      "1. You need to frequently insert or delete elements at arbitrary positions.\n",
      "2. You need to efficiently traverse the list by following pointers from one node to the next.\n",
      "3. Memory is a concern, and you want to minimize memory usage (e.g., for large datasets).\n",
      "\n",
      "Some key characteristics of linked lists include:\n",
      "\n",
      "* **Dynamic size**: Linked lists can grow or shrink dynamically as elements are added or removed.\n",
      "* **Efficient insertion/deletion**: Inserting or deleting an element at the end of a linked list takes constant time (O(1)). For arbitrary positions, it's still relatively efficient (O(n), where n is the number of elements).\n",
      "* **No indexing**: Unlike arrays, linked lists don't have indices to keep track of the position of each element.\n",
      "\n",
      "Common types of linked lists include:\n",
      "\n",
      "1. **Singly linked list**: Each node points only to the next one in the sequence.\n",
      "2. **Doubly linked list**: Each node points both to the previous and next ones in the sequence.\n",
      "3. **Circular linked list**: The last node points back to the first one, forming a circular structure.\n",
      "\n",
      "Do you have any specific questions about linked lists or would you like an example implementation?\n",
      "Here's an example of a basic singly-linked list implementation in Python:\n",
      "\n",
      "```python\n",
      "class Node:\n",
      "    \"\"\"Represents a single node in the linked list.\"\"\"\n",
      "    def __init__(self, data=None):\n",
      "        self.data = data\n",
      "        self.next = None\n",
      "\n",
      "\n",
      "class LinkedList:\n",
      "    \"\"\"Represents the entire linked list.\"\"\"\n",
      "    def __init__(self):\n",
      "        self.head = None\n",
      "\n",
      "    def insert_at_head(self, data):\n",
      "        \"\"\"Inserts a new node with `data` at the head of the list.\"\"\"\n",
      "        new_node = Node(data)\n",
      "        new_node.next = self.head\n",
      "        self.head = new_node\n",
      "\n",
      "    def insert_at_tail(self, data):\n",
      "        \"\"\"Inserts a new node with `data` at the tail of the list.\"\"\"\n",
      "        if not self.head:\n",
      "            self.insert_at_head(data)\n",
      "            return\n",
      "        current = self.head\n",
      "        while current.next:\n",
      "            current = current.next\n",
      "        current.next = Node(data)\n",
      "\n",
      "    def delete(self, data):\n",
      "        \"\"\"Deletes the first occurrence of a node with `data` in the list.\"\"\"\n",
      "        if self.head is None:\n",
      "            return\n",
      "\n",
      "        # Check if we need to remove the head\n",
      "        if self.head.data == data:\n",
      "            self.head = self.head.next\n",
      "            return\n",
      "\n",
      "        current = self.head\n",
      "        while current.next:\n",
      "            if current.next.data == data:\n",
      "                current.next = current.next.next\n",
      "                return\n",
      "            current = current.next\n",
      "\n",
      "    def print_list(self):\n",
      "        \"\"\"Prints the elements of the linked list.\"\"\"\n",
      "        current = self.head\n",
      "        while current:\n",
      "            print(current.data, end=\" \")\n",
      "            current = current.next\n",
      "        print()\n",
      "\n",
      "# Example usage:\n",
      "linked_list = LinkedList()\n",
      "linked_list.insert_at_head(1)\n",
      "linked_list.insert_at_tail(2)\n",
      "linked_list.insert_at_head(0)\n",
      "print(\"Linked List:\")\n",
      "linked_list.print_list()  # Output: 0 1 2\n",
      "\n",
      "linked_list.delete(2)\n",
      "print(\"After deleting 2:\")\n",
      "linked_list.print_list()  # Output: 0 1\n",
      "```\n",
      "\n",
      "This implementation includes:\n",
      "\n",
      "*   `Node` class for representing individual nodes in the list.\n",
      "*   `LinkedList` class with methods for:\n",
      "    *   `insert_at_head`: Inserting a new node at the head of the list.\n",
      "    *   `insert_at_tail`: Inserting a new node at the tail of the list.\n",
      "    *   `delete`: Deleting the first occurrence of a node with specified data in the list.\n",
      "    *   `print_list`: Printing the elements of the linked list.\n",
      "\n",
      "Feel free to ask if you'd like me to clarify or modify anything!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "]\n",
    "\n",
    "def ask(prompt):\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = ollama.chat(model=\"llama3.1:8b\", messages=chat_history)\n",
    "    answer = response['message']['content']\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    return answer\n",
    "\n",
    "print(ask(\"What is a linked list?\"))\n",
    "print(ask(\"Give the code for it in Python\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6612159",
   "metadata": {},
   "source": [
    "Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe28ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extractive Summary -----\n",
      "Here is a summary of the text using extractive summarization:\n",
      "\n",
      "**Ollama Framework**\n",
      "\n",
      "Ollama allows developers to run large language models (LLMs) locally on their computers. It supports popular models like LLaMA, Mistral, and Gemma, offering advantages such as privacy, cost savings, and offline development. However, it requires a sufficient local hardware setup with modern CPU and GPU resources.\n",
      "\n",
      "----- Abstractive Summary -----\n",
      "Here is a summary of the text using abstractive summarization:\n",
      "\n",
      "\"Ollama enables developers to seamlessly run large language models on their own machines, offering unparalleled flexibility and control over model deployment without the need for cloud services or excessive hardware.\"\n",
      "\n",
      "Note that abstractive summarization aims to condense the original text into a shorter, more readable summary while also attempting to preserve its core meaning and essence. This requires some level of interpretation and synthesis of the original content.\n"
     ]
    }
   ],
   "source": [
    "# Original passage\n",
    "text = \"\"\"\n",
    "Ollama is an open-source framework that allows developers to run large language models (LLMs) locally on their computers. \n",
    "It supports popular models like LLaMA, Mistral, and Gemma. \n",
    "With Ollama, you don't need to rely on cloud services, which makes it useful for privacy, cost savings, and offline development. \n",
    "However, it requires sufficient local hardware resources, such as a modern CPU and GPU.\n",
    "\"\"\"\n",
    "\n",
    "# Extractive summary prompt\n",
    "extractive_prompt = f\"\"\"\n",
    "Summarize the following text using extractive summarization. \n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Abstractive summary prompt\n",
    "abstractive_prompt = f\"\"\"\n",
    "Summarize the following text using abstractive summarization. \n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "print(\"----- Extractive Summary -----\")\n",
    "print(ask(extractive_prompt))\n",
    "\n",
    "print(\"\\n----- Abstractive Summary -----\")\n",
    "print(ask(abstractive_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d4964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
