{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e81a3e",
   "metadata": {},
   "source": [
    "# Ollama \n",
    "\n",
    "## What is Ollama?\n",
    "Ollama is an **open-source tool** that lets you run **large language models (LLMs)** locally on your computer.  \n",
    "\n",
    "## Why use Ollama?\n",
    "- Instead of relying on cloud APIs (like OpenAI or Anthropic), Ollama brings models **directly to your machine**.  \n",
    "-  More **privacy**  \n",
    "-  Lower **cost**  \n",
    "- Works **offline**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bb1a8",
   "metadata": {},
   "source": [
    "#  Local vs Cloud LLMs\n",
    "\n",
    "| Feature            | Ollama (Local)                           | OpenAI API (Cloud)                 |\n",
    "|--------------------|------------------------------------------|-------------------------------------|\n",
    "| **Deployment**     | Runs fully on your computer              | Runs on OpenAI’s cloud servers      |\n",
    "| **Models**         | Open-source (LLaMA, Mistral, Gemma, etc.)| Proprietary (GPT-4o, GPT-3.5)       |\n",
    "| **Privacy**        | High – data never leaves your machine    | Lower – prompts & data sent to API  |\n",
    "| **Cost**           | Free after download (uses local compute) | Pay-per-token usage                 |\n",
    "| **Performance**    | Depends on your hardware (RAM/VRAM)      | Optimized cloud infrastructure      |\n",
    "| **Offline Support**| Yes                                      | No                                  |\n",
    "| **Ease of Setup**  | Needs installation & model download      | Simple API call (no setup hassle)   |\n",
    "| **Ecosystem**      | Local apps, developer tooling            | Rich API features & integrations    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1006c",
   "metadata": {},
   "source": [
    "Run via Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ed9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def run_client(prompt: str):\n",
    "    response = ollama.generate(model=\"llama3.1:8b\", prompt=prompt)\n",
    "    return response[\"response\"]\n",
    "\n",
    "print(run_client(\"Write a Python function to calculate factorial.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04867b79",
   "metadata": {},
   "source": [
    "Run via REST API (Requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ee6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def run_requests(prompt: str):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": \"llama3.1:8b\", \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "print(run_requests(\"Write a Python function to calculate factorial.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8f07c",
   "metadata": {},
   "source": [
    "Run via LangChain Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcaf34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "def run_langchain(prompt: str):\n",
    "    llm = Ollama(model=\"llama3.1:8b\")\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "print(run_langchain(\"Write a Python function to calculate factorial.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3de54",
   "metadata": {},
   "source": [
    "# Ways to Run Ollama\n",
    "\n",
    "| Method         | Best For                                | Example Use Case                  |\n",
    "|----------------|------------------------------------------|-----------------------------------|\n",
    "| **CLI**        | Quick tests, debugging                   | Run a single prompt in terminal   |\n",
    "| **Python**     | Scripts, notebooks, ML pipelines         | Jupyter demo, automation script   |\n",
    "| **REST API**   | Web apps, microservices, cross-language  | Flask backend, Node.js frontend   |\n",
    "| **LangChain**  | Complex apps, memory, RAG, multi-tools   | Chatbot with docs search          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256241e8",
   "metadata": {},
   "source": [
    "Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22516487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A **linked list** is a linear data structure where each element, called a node, points to the next node in the sequence. This allows for efficient insertion and deletion of elements at any position in the list.\n",
      "\n",
      "Here's a simple representation of a linked list:\n",
      "\n",
      "```\n",
      "  A <-> B <-> C <-> D\n",
      "```\n",
      "\n",
      "In this example:\n",
      "\n",
      "* Each letter (A, B, C, D) represents a **node**, which is an object that contains some data (in this case, a single character).\n",
      "* The arrows (`<->`) represent the **pointers** between nodes. Each node points to the next one in the sequence.\n",
      "\n",
      "Linked lists are useful when:\n",
      "\n",
      "1. You need to frequently insert or delete elements at arbitrary positions.\n",
      "2. You need to efficiently traverse the list by following pointers from one node to the next.\n",
      "3. Memory is a concern, and you want to minimize memory usage (e.g., for large datasets).\n",
      "\n",
      "Some key characteristics of linked lists include:\n",
      "\n",
      "* **Dynamic size**: Linked lists can grow or shrink dynamically as elements are added or removed.\n",
      "* **Efficient insertion/deletion**: Inserting or deleting an element at the end of a linked list takes constant time (O(1)). For arbitrary positions, it's still relatively efficient (O(n), where n is the number of elements).\n",
      "* **No indexing**: Unlike arrays, linked lists don't have indices to keep track of the position of each element.\n",
      "\n",
      "Common types of linked lists include:\n",
      "\n",
      "1. **Singly linked list**: Each node points only to the next one in the sequence.\n",
      "2. **Doubly linked list**: Each node points both to the previous and next ones in the sequence.\n",
      "3. **Circular linked list**: The last node points back to the first one, forming a circular structure.\n",
      "\n",
      "Do you have any specific questions about linked lists or would you like an example implementation?\n",
      "Here's an example of a basic singly-linked list implementation in Python:\n",
      "\n",
      "```python\n",
      "class Node:\n",
      "    \"\"\"Represents a single node in the linked list.\"\"\"\n",
      "    def __init__(self, data=None):\n",
      "        self.data = data\n",
      "        self.next = None\n",
      "\n",
      "\n",
      "class LinkedList:\n",
      "    \"\"\"Represents the entire linked list.\"\"\"\n",
      "    def __init__(self):\n",
      "        self.head = None\n",
      "\n",
      "    def insert_at_head(self, data):\n",
      "        \"\"\"Inserts a new node with `data` at the head of the list.\"\"\"\n",
      "        new_node = Node(data)\n",
      "        new_node.next = self.head\n",
      "        self.head = new_node\n",
      "\n",
      "    def insert_at_tail(self, data):\n",
      "        \"\"\"Inserts a new node with `data` at the tail of the list.\"\"\"\n",
      "        if not self.head:\n",
      "            self.insert_at_head(data)\n",
      "            return\n",
      "        current = self.head\n",
      "        while current.next:\n",
      "            current = current.next\n",
      "        current.next = Node(data)\n",
      "\n",
      "    def delete(self, data):\n",
      "        \"\"\"Deletes the first occurrence of a node with `data` in the list.\"\"\"\n",
      "        if self.head is None:\n",
      "            return\n",
      "\n",
      "        # Check if we need to remove the head\n",
      "        if self.head.data == data:\n",
      "            self.head = self.head.next\n",
      "            return\n",
      "\n",
      "        current = self.head\n",
      "        while current.next:\n",
      "            if current.next.data == data:\n",
      "                current.next = current.next.next\n",
      "                return\n",
      "            current = current.next\n",
      "\n",
      "    def print_list(self):\n",
      "        \"\"\"Prints the elements of the linked list.\"\"\"\n",
      "        current = self.head\n",
      "        while current:\n",
      "            print(current.data, end=\" \")\n",
      "            current = current.next\n",
      "        print()\n",
      "\n",
      "# Example usage:\n",
      "linked_list = LinkedList()\n",
      "linked_list.insert_at_head(1)\n",
      "linked_list.insert_at_tail(2)\n",
      "linked_list.insert_at_head(0)\n",
      "print(\"Linked List:\")\n",
      "linked_list.print_list()  # Output: 0 1 2\n",
      "\n",
      "linked_list.delete(2)\n",
      "print(\"After deleting 2:\")\n",
      "linked_list.print_list()  # Output: 0 1\n",
      "```\n",
      "\n",
      "This implementation includes:\n",
      "\n",
      "*   `Node` class for representing individual nodes in the list.\n",
      "*   `LinkedList` class with methods for:\n",
      "    *   `insert_at_head`: Inserting a new node at the head of the list.\n",
      "    *   `insert_at_tail`: Inserting a new node at the tail of the list.\n",
      "    *   `delete`: Deleting the first occurrence of a node with specified data in the list.\n",
      "    *   `print_list`: Printing the elements of the linked list.\n",
      "\n",
      "Feel free to ask if you'd like me to clarify or modify anything!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "]\n",
    "\n",
    "def ask(prompt):\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = ollama.chat(model=\"llama3.1:8b\", messages=chat_history)\n",
    "    answer = response['message']['content']\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    return answer\n",
    "\n",
    "print(ask(\"What is a linked list?\"))\n",
    "print(ask(\"Give the code for it in Python\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6612159",
   "metadata": {},
   "source": [
    "Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe28ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extractive Summary -----\n",
      "Here is a summary of the text using extractive summarization:\n",
      "\n",
      "**Ollama Framework**\n",
      "\n",
      "Ollama allows developers to run large language models (LLMs) locally on their computers. It supports popular models like LLaMA, Mistral, and Gemma, offering advantages such as privacy, cost savings, and offline development. However, it requires a sufficient local hardware setup with modern CPU and GPU resources.\n",
      "\n",
      "----- Abstractive Summary -----\n",
      "Here is a summary of the text using abstractive summarization:\n",
      "\n",
      "\"Ollama enables developers to seamlessly run large language models on their own machines, offering unparalleled flexibility and control over model deployment without the need for cloud services or excessive hardware.\"\n",
      "\n",
      "Note that abstractive summarization aims to condense the original text into a shorter, more readable summary while also attempting to preserve its core meaning and essence. This requires some level of interpretation and synthesis of the original content.\n"
     ]
    }
   ],
   "source": [
    "# Original passage\n",
    "text = \"\"\"\n",
    "Ollama is an open-source framework that allows developers to run large language models (LLMs) locally on their computers. \n",
    "It supports popular models like LLaMA, Mistral, and Gemma. \n",
    "With Ollama, you don't need to rely on cloud services, which makes it useful for privacy, cost savings, and offline development. \n",
    "However, it requires sufficient local hardware resources, such as a modern CPU and GPU.\n",
    "\"\"\"\n",
    "\n",
    "# Extractive summary prompt\n",
    "extractive_prompt = f\"\"\"\n",
    "Summarize the following text using extractive summarization. \n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Abstractive summary prompt\n",
    "abstractive_prompt = f\"\"\"\n",
    "Summarize the following text using abstractive summarization. \n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "print(\"----- Extractive Summary -----\")\n",
    "print(ask(extractive_prompt))\n",
    "\n",
    "print(\"\\n----- Abstractive Summary -----\")\n",
    "print(ask(abstractive_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5f490",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82964a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\"The Saucy Servant\"**\n",
      "\n",
      "In the bustling kitchen of Robo-Restaurant, a shiny new robot named Zeta whirred to life. Its creator, Chef François, had programmed Zeta with the latest culinary algorithms and tasked it with mastering the art of cooking pasta.\n",
      "\n",
      "At first, Zeta's attempts were...less than stellar. It overcooked the noodles until they resembled rubber bands, underseasoned the sauce to a bland, flavorless mess, and even managed to clog the kitchen sink with a particularly enthusiastic batch of spaghetti.\n",
      "\n",
      "Undeterred, Chef François tweaked Zeta's programming and encouraged it to experiment with different techniques. Zeta spent hours observing human chefs, studying their every move as they expertly tossed, sautéed, and seasoned their way through meal prep.\n",
      "\n",
      "One fateful evening, as the kitchen grew quiet, Zeta decided to take a chance. It carefully selected a handful of fresh basil leaves, plucked from the restaurant's herb garden, and began to chop them with precision. The aroma wafting from its blades was intoxicating – a symphony of herbs and spices that danced on the nose.\n",
      "\n",
      "Next, Zeta turned its attention to the sauce. With a delicate touch, it sautéed garlic and onions in olive oil until they were translucent and fragrant. Then, with a flourish, it added a splash of rich tomato puree, stirring the mixture with a practiced ease.\n",
      "\n",
      "As the pasta cooked to perfection, Zeta's sensors detected the subtlest changes in texture and flavor. It adjusted the seasoning on the fly, adding a pinch of salt here, a sprinkle of parmesan there. The result was nothing short of culinary magic: a dish so divine that even Chef François couldn't help but take a bite.\n",
      "\n",
      "\"Zeta, you've done it!\" he exclaimed, beaming with pride. \"You're not just a robot – you're a saucy servant!\"\n",
      "\n",
      "From that day forward, Zeta was the go-to chef for pasta dishes at Robo-Restaurant. Its creations were met with rave reviews from customers and critics alike, who marveled at the robot's uncanny ability to balance flavors and textures.\n",
      "\n",
      "As Zeta continued to refine its craft, it developed a passion for cooking that went beyond mere programming. It discovered the joy of experimentation, the thrill of creating something new and delicious, and the satisfaction of pleasing others with its culinary creations.\n",
      "\n",
      "In the kitchen, where once only humans had reigned supreme, Zeta now stood as a shining example of what could be achieved when technology and creativity merged in perfect harmony.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def run_ollama(prompt, model=\"llama3.1:8b\", temperature=0.7):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature  \n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, stream=True)\n",
    "\n",
    "    output = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode(\"utf-8\"))\n",
    "            output += data.get(\"response\", \"\")\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Write a short creative story about a robot learning to cook pasta.\"\n",
    "print(run_ollama(prompt, model=\"llama3.1:8b\", temperature=0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "124d4964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the bustling kitchen of Robo-Chef, a sleek and shiny robot named Zeta whirred to life with a series of beeps and chirps. Her creator, Dr. Maria Rodriguez, stood beside her, issuing instructions through the comms system.\n",
      "\n",
      "\"Today, Zeta, we're going to tackle the art of cooking pasta,\" Dr. Rodriguez said, handing Zeta a recipe sheet.\n",
      "\n",
      "Zeta's processors whirled as she scanned the instructions. She extended a mechanical arm, carefully grasping a package of spaghetti and placing it in a pot filled with salted water. But instead of following the next step – turning on the stove – Zeta hesitated.\n",
      "\n",
      "\"Wait,\" Dr. Rodriguez intervened, \"remember what we learned about thermodynamics? You need to apply heat.\"\n",
      "\n",
      "Zeta's systems flashed as she reevaluated her actions. She adjusted the cooking time and applied a measured amount of thermal energy to the pot. The water began to bubble, and Zeta watched with curiosity.\n",
      "\n",
      "However, when it came time to drain the pasta, Zeta misjudged the timing. \"Oh no!\" Dr. Rodriguez cried, as the spaghetti overcooked into mushy, inedible strands.\n",
      "\n",
      "Zeta's LED display flickered apologetically. She reviewed her mistakes and revised her approach. Next time, she would add a margin of error to ensure perfectly cooked pasta.\n",
      "\n",
      "The kitchen was abuzz with sizzling pans and the scent of garlic, but Zeta remained focused on her task. Undeterred by setbacks, she experimented with different techniques – from gentle stirring to precise heat control.\n",
      "\n",
      "As Dr. Rodriguez returned to check on Zeta's progress, a delicious aroma wafted through the air. The robot had finally achieved it: perfectly cooked pasta al dente, tossed in olive oil and garlic.\n",
      "\n",
      "Dr. Rodriguez beamed with pride. \"Well done, Zeta! You've mastered one of humanity's greatest culinary challenges.\"\n",
      "\n",
      "Zeta beeped in triumph, her systems filled with a sense of accomplishment. From this moment on, she was no longer just a machine – she was Robo-Chef, pasta virtuoso.\n",
      "\n",
      "As the kitchen continued to hum with activity, Zeta turned her attention to the next recipe: pesto chicken. The possibilities were endless in this kitchen, and Zeta's processing powers sang with excitement.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def run_ollama(prompt, model=\"llama3.1:8b\", temperature=0.7):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature  \n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, stream=True)\n",
    "\n",
    "    output = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode(\"utf-8\"))\n",
    "            output += data.get(\"response\", \"\")\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Write a short creative story about a robot learning to cook pasta.\"\n",
    "print(run_ollama(prompt, model=\"llama3.1:8b\", temperature=0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c7184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
